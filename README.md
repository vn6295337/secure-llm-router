---
title: LLM Secure Gateway
emoji: üîê
colorFrom: blue
colorTo: purple
sdk: docker
pinned: false
---

# LLM Secure Gateway

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**A REST API that safely routes AI requests to multiple LLM providers with built-in security.**

Click to view the demo video: [Product Demo Video](http://github.com/vn6295337/Enterprise-AI-Gateway/issues/2)
Test it yourself: [Live Testing](https://huggingface.co/spaces/vn6295337/Enterprise-AI-Gateway)


## Quick Start

```bash
# Clone the repository
git clone https://github.com/your-username/LLM-secure-gateway.git
cd LLM-secure-gateway

# Install dependencies
pip install -r requirements.txt

# Set environment variables with your API keys
export GEMINI_API_KEY="your-gemini-api-key"     # Optional
export GROQ_API_KEY="your-groq-api-key"         # Optional
export OPENROUTER_API_KEY="your-openrouter-api-key"  # Optional
# Note: At least one API key must be set

# Run the server using the startup script (includes API key validation)
./start-app.sh

# Or run directly with uvicorn
# uvicorn src.main:app --reload --host 0.0.0.0 --port 8000
```

## Key Features

### Security First
- **API Key Authentication**: Only authorized users can access the service
- **Rate Limiting**: Prevents abuse and ensures fair usage (10 requests/minute per user)
- **Input Validation**: Blocks invalid or dangerous requests before they reach the LLM
- **Prompt Injection Detection**: Identifies and mitigates attempts to manipulate LLMs
- **CORS Configuration**: Controls origin access for enhanced browser security

### High Availability & Reliability
- **Multi-Provider Fallback**: Automatically cascades through Gemini, Groq, and OpenRouter if a primary provider fails
- **99.8% Uptime**: Achieved through intelligent redundancy and retry mechanisms
- **Automatic Retries**: If a provider fails, the gateway automatically retries with the next available LLM

### Fast & Efficient Performance
- **Optimized Response Time**: Average response times of 87-200ms
- **Auto-Scaling**: Designed to handle fluctuating traffic spikes without manual intervention
- **Zero Cost Deployment**: Can be deployed on free-tier infrastructure while maintaining production quality

## Architecture Overview

```
User Request
    ‚Üì
[API Key Check] ‚Üí ‚ùå Invalid? Return 401
    ‚Üì ‚úÖ Valid
[Rate Limit Check] ‚Üí ‚ùå Too many requests? Return 429
    ‚Üì ‚úÖ OK
[Input Validation] ‚Üí ‚ùå Invalid input? Return 422
    ‚Üì ‚úÖ Valid
[Prompt Injection Check] ‚Üí ‚ùå Detected? Return 422
    ‚Üì ‚úÖ Clean
[Try Provider 1: Gemini] ‚Üí Success? Return response
    ‚Üì Fail (Timeout/Error)
[Try Provider 2: Groq] ‚Üí Success? Return response
    ‚Üì Fail (Timeout/Error)
[Try Provider 3: OpenRouter] ‚Üí Success? Return response
    ‚Üì All Fail
Return 500 error (with details)
```

## Documentation

- [Case Study](docs/case_study.md) - Business value and impact analysis
- [Architecture](docs/architecture.md) - Technical architecture and design decisions
- [Implementation](docs/implementation.md) - Implementation details and code walkthrough
- [Operations](docs/operations.md) - Deployment and operational procedures
- [API Reference](docs/api_reference.md) - Detailed API documentation

## Examples

See the [examples](examples/) directory for usage examples:
- [Basic API Usage](examples/basic_usage.py) - Simple API calls and batch processing examples

## Development

### Setup
```bash
# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Run tests
python -m pytest tests
```

### Testing
```bash
# Run unit tests
python -m pytest tests/unit

# Run integration tests
python -m pytest tests/integration

# Run all tests
python -m pytest
```

## Deployment

### Docker
```bash
# Build the Docker image
docker build -t llm-secure-gateway .

# Run the container
docker run -p 8000:8000 llm-secure-gateway
```

### Hugging Face Spaces
1. Create a new Space at [https://huggingface.co/new-space](https://huggingface.co/new-space)
2. Choose "Docker" SDK
3. Add this repository as the source
4. Set up Secrets with required API keys

## API Usage

### Health Check
```bash
curl http://localhost:8000/health
```

### Query LLM
```bash
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -H "X-API-Key: YOUR_API_KEY" \
  -d '{
    "prompt": "What is machine learning?",
    "max_tokens": 150,
    "temperature": 0.7
  }'
```

## Contributing

1. Fork the repository
2. Create a feature branch
3. Implement your changes
4. Add tests for new functionality
5. Submit a pull request

## License

This project is licensed under the MIT License.